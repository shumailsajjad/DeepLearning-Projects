{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97ab5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7fb180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_IN = \"FILL_IN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c0320b",
   "metadata": {},
   "source": [
    "### Get the data and process\n",
    "- This is the Mysterious island found in Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4e64a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1130711\n",
      "Unique Characters: 85\n"
     ]
    }
   ],
   "source": [
    "## Reading and processing text\n",
    "with open('data/1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text=fp.read()\n",
    "\n",
    "# Get the index of 'THE MYSTERIOUS ISLAND' or 'The Mysterious Island'\n",
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "# Get the index of 'End of the Project Gutenberg'\n",
    "end_indx = text.find('END of the Project Gutenberg')\n",
    "\n",
    "# Set text to the text between start and end idx.\n",
    "text = text[start_indx:end_indx]\n",
    "# Get the unique set of characters.\n",
    "char_set = set(text)\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Characters:', len(char_set))\n",
    "assert(len(text) == 1130711)\n",
    "assert(len(char_set) == 85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76393bdb",
   "metadata": {},
   "source": [
    "### Tokenze and get other helpers\n",
    "- We do this manually since everything is character based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a445114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (1130711,)\n",
      "THE MYSTERIOUS       == Encoding ==>  [48 36 33  1 41 53 47 48 33 46 37 43 49 47  1]\n",
      "[37 47 40 29 42 32]  == Reverse  ==>  ISLAND\n"
     ]
    }
   ],
   "source": [
    "# The universe of words.\n",
    "chars_sorted = sorted(char_set)\n",
    "\n",
    "# Effectively, these maps are the tokenizer.\n",
    "# Map each char to a unique int. This is a dict.\n",
    "char2int = {char: i for i, char in enumerate(chars_sorted)}\n",
    "# Do the revverse of the above, this should be a np array.\n",
    "int2char =  np.array(chars_sorted)\n",
    "\n",
    "# Tokenize the entire corpus. This should be an np array of np.int32 type.\n",
    "text_encoded = np.array([char2int[char] for char in text], dtype=np.int32)\n",
    "\n",
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "\n",
    "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
    "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(int2char[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720cd752",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2743a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (1130711,)\n",
      "THE MYSTERIOUS       == Encoding ==>  [48 36 33  1 41 53 47 48 33 46 37 43 49 47  1]\n",
      "[37 47 40 29 42 32]  == Reverse  ==>  ISLAND\n"
     ]
    }
   ],
   "source": [
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
    "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(int2char[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "367e733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(\n",
    "    np.array_equal(\n",
    "    text_encoded[:15],\n",
    "        [48, 36, 33, 1, 41, 53, 47, 48, 33, 46, 37, 43, 49, 47,  1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c418ca0",
   "metadata": {},
   "source": [
    "### Process the data and get the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f429dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "# Break up the data into chunks of size 41. This should be a list of lists.\n",
    "# Use text_encoded. This will be used to get (x, y) pairs.\n",
    "text_chunks = [text_encoded[i:i + chunk_size] for i in range(0, len(text_encoded) - chunk_size + 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e329fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the text chunk at index idx.\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        # Return (x, y) where x has length 40 and y has length 40.\n",
    "        # y should be x shifted by 1 time.\n",
    "        return text_chunk[:-1], text_chunk[1:]\n",
    "    \n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71328555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40]) torch.Size([40])\n",
      "Input (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'\n",
      "Target (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "\n",
      "torch.Size([40]) torch.Size([40])\n",
      "Input (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "Target (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERIO'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    # 40 characters for source and target ...\n",
    "    print(seq.shape, target.shape)\n",
    "    print('Input (x):', repr(''.join(int2char[seq])))\n",
    "    print('Target (y):', repr(''.join(int2char[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebb989c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a881b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed0b2f",
   "metadata": {},
   "source": [
    "### Write the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b4cbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        # Set to an embedding layer of vocab_size by embed_dim.\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) \n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        # Set to an LSTM with x having embed_dim and h dimension rnn_hidden_size.\n",
    "        # batch_first shoould be true.\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        \n",
    "        # Make a linear layer from rnn_hidden_size to vocab_size.\n",
    "        # This will be used to get the yt for each xt.\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, text, hidden=None, cell=None):\n",
    "        # Get the embeddings for text.\n",
    "        out = self.embedding(text)\n",
    "        \n",
    "        # Pass out, hidden and cell through the rnn.\n",
    "        # If hidden is None, don't specify it and just use out.\n",
    "        if hidden is not None:\n",
    "            out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        else:\n",
    "            out, (hidden, cell) = self.rnn(out)\n",
    "        \n",
    "        # Pass out through fc.\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, (hidden, cell)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize to zeros of 1 by ??? appropriate dimensions.\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden.to(device), cell.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00789dfd",
   "metadata": {},
   "source": [
    "### Do this right way - across all data all at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33380607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(85, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(int2char)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f47f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.4364\n",
      "Epoch 100 loss: 1.7578\n",
      "Epoch 200 loss: 1.5684\n",
      "Epoch 300 loss: 1.4286\n",
      "Epoch 400 loss: 1.4797\n",
      "Epoch 500 loss: 1.3564\n",
      "Epoch 600 loss: 1.3440\n",
      "Epoch 700 loss: 1.3495\n",
      "Epoch 800 loss: 1.3161\n",
      "Epoch 900 loss: 1.2960\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Set to 10000.\n",
    "num_epochs = 1000\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# epochs here will mean batches.\n",
    "# If the above takes too long, use 1000.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    \n",
    "    # Get the next batch from seq_dl\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "        \n",
    "    seq_batch = seq_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    # Pass through the model.\n",
    "    logits, _ = model(seq_batch, hidden, cell) \n",
    "    \n",
    "    # Get the loss.\n",
    "    # You'll need to reshape / view things to make this work.\n",
    "    loss += criterion(logits.transpose(1, 2), target_batch.long())\n",
    "        \n",
    "    # Do back prop.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    # Get the value in the tensor loss.\n",
    "    loss = loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f398f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: tensor([[0.0159, 0.1173, 0.8668]])\n",
      "[[1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "logits = torch.tensor([[-1.0, 1.0, 3.0]])\n",
    "\n",
    "# Get the probabilities for these logits.\n",
    "print('Probabilities:', logits.softmax(dim=-1))\n",
    "\n",
    "# Get a Categorical random variable with the above probabilities for each of the classes.\n",
    "m = Categorical(logits.softmax(dim=-1))\n",
    "# Generate 10 things.\n",
    "samples = m.sample(sample_shape=torch.Size([10]))\n",
    " \n",
    "print(samples.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0547467d",
   "metadata": {},
   "source": [
    "### Random decoding.\n",
    "- This compounds problems: once you make a mistake, you can't undo it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "614fb236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island had build ane used a different mingly.\n",
      "\n",
      "“Anglo, audd on writted Harding, “orded on the\n",
      "moons of the rich bime direction of the mind; the “Bonadventure!” said Pencroft.\n",
      "\n",
      "““The shunger soon an’th the trees. The reporter wheen, Ayrton or exactly\n",
      "through the currently case, cried the settlers in the\n",
      "purpost in such year, which\n",
      "they she or descent of the sand. Their discharge collections escaped, and\n",
      "fallows of his shoopes and soon on the lad been massing the most carning with will in a\n",
      "hander sprin\n"
     ]
    }
   ],
   "source": [
    "def random_sample(\n",
    "    model,\n",
    "    starting_str, \n",
    "    len_generated_text=500, \n",
    "):\n",
    "\n",
    "    # Encode starting string into a tensor using char2str.\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
    "    \n",
    "    # Reshape to be 1 by ??? - let PyTorch figure this out.\n",
    "    encoded_input = encoded_input.unsqueeze(0)\n",
    "\n",
    "    # This will be what you generate, but it starts off with something.\n",
    "    generated_str = starting_str\n",
    "\n",
    "    # Put model in eval mode. This matters if we had dropout o batch / layer norms.\n",
    "    model.eval()\n",
    "    \n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    \n",
    "    hidden = hidden.to(device)\n",
    "    \n",
    "    cell = cell.to(device)\n",
    "        \n",
    "    # Build up the starting hidden and cell states.\n",
    "    # You can do this all in one go?\n",
    "    for c in range(len(starting_str)-1):\n",
    "        # Feed each letter 1 by 1 and then get the final hidden state.\n",
    "        out = encoded_input[:, c:c+1]\n",
    "        # Pass out through, note we update hidden and cell and use them again\n",
    "        _, (hidden, cell) = model(out, hidden, cell)\n",
    "    \n",
    "    # Gte the last char; note we did not do go to the last char above.\n",
    "    last_char = encoded_input[:, -1:]\n",
    "\n",
    "    # Generate chars one at a time, add them to generated_str.\n",
    "    # Do this over and over until you get the desired length.\n",
    "    for i in range(len_generated_text):\n",
    "        \n",
    "        # Use hidden and cell from the above.\n",
    "        # Use last_char, which will be updated over and over.\n",
    "        logits, (hidden, cell) = model(last_char, hidden, cell)\n",
    "        \n",
    "        # Get the logits.\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # m is a random variable with probabilities based on the softmax of the logits.\n",
    "        m = Categorical(logits.softmax(dim=-1))\n",
    "        \n",
    "        # Generate from m 1 char.\n",
    "        last_char = m.sample().view(1,1)\n",
    "\n",
    "        # Add the geenrated char to generated_str, but pass it through int2str so that \n",
    "        generated_str += int2char[last_char.item()]\n",
    "        \n",
    "    return generated_str\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model.to(device)\n",
    "print(random_sample(model, starting_str='The island'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
